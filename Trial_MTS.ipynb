{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0hRFxRFwcEvN3+Pr2rIrL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hussein-Ahmad-94/Tasks/blob/main/Trial_MTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "import tensorflow.keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "df=pd.read_csv(\"/content/train.csv\",parse_dates=[\"Date\"],index_col=[0])\n",
        "print(df.index.freq)\n",
        "df.shape\n",
        "df.head()\n",
        "df.tail()\n",
        "\n",
        "x1 = df.iloc[:,0]\n",
        "x2 = df.iloc[:,1]\n",
        "x3 = df.iloc[:,2]\n",
        "x4 = df.iloc[:,3]\n",
        "x5 = df.iloc[:,4]\n",
        "\n",
        "input_data = np.concatenate([x2,x3,x4,x5], axis=-1)\n",
        "\n",
        "\n",
        "input_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVK5RFvz8aLH",
        "outputId": "60618fcd-4363-4a22-d7aa-0afc8a7674b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([362.980774, 360.096161, 355.769226, ..., 105.257835, 105.637512,\n",
              "       106.187027])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming x2, x3, x4, x5 are pandas Series\n",
        "# Convert them to numpy arrays using the .values attribute\n",
        "\n",
        "x2_array = x2.values.reshape(-1, 1)  # Reshape to a column vector\n",
        "x3_array = x3.values.reshape(-1, 1)\n",
        "x4_array = x4.values.reshape(-1, 1)\n",
        "x5_array = x5.values.reshape(-1, 1)\n",
        "\n",
        "future_steps = 3\n",
        "\n",
        "# Now concatenate the arrays\n",
        "y_true = np.concatenate([x2_array[-future_steps:, :], x3_array[-future_steps:, :], x4_array[-future_steps:, :], x5_array[-future_steps:, :]], axis=-1)\n"
      ],
      "metadata": {
        "id": "GGbnqUU0Aia8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom loss function\n",
        "def custom_loss(y_true, y_pred):\n",
        "    y_pred_expanded = K.expand_dims(y_pred, -1)  # Expanding the last dimension of y_pred\n",
        "    loss_x2 = K.mean(K.square(y_pred_expanded - y_true[:, :, :1]))  # Referencing the second feature from y_true\n",
        "    loss_x3 = K.mean(K.square(y_pred_expanded - y_true[:, :, 1:2]))  # Referencing the third feature from y_true\n",
        "    loss_x4 = K.mean(K.square(y_pred_expanded - y_true[:, :, 2:3]))  # Referencing the fourth feature from y_true\n",
        "    loss_x5 = K.mean(K.square(y_pred_expanded - y_true[:, :, 3:4]))  # Referencing the fifth feature from y_true\n",
        "    total_loss = loss_x2 + loss_x3 + loss_x4 + loss_x5\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "yxutciVIAyeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "input_layer = Input(shape=(time_steps, 2))\n",
        "x = LSTM(64, return_sequences=True)(input_layer)\n",
        "x = LSTM(64)(x)\n",
        "output_layer = Dense(future_steps)(x)\n"
      ],
      "metadata": {
        "id": "-Q6Y04SRBrkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model with the custom loss function\n",
        "model.compile(optimizer=Adam(), loss=custom_loss)\n",
        "\n"
      ],
      "metadata": {
        "id": "TImT59c2BvGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming input_data is 1-dimensional\n",
        "# Reshape it to have two dimensions: (number of samples, number of features)\n",
        "input_data = input_data.reshape((input_data.shape[0], 1))\n",
        "\n",
        "# Now, add a third dimension for timesteps (assuming you have one timestep per sample)\n",
        "input_data = np.expand_dims(input_data, axis=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(input_data, y_true, epochs=3, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "xg-QK3xsC0Cj",
        "outputId": "7f94339f-6ae0-4b69-d023-ece4b1de0e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-92a31b122cf2>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 10, 2), found shape=(None, 1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cutt**"
      ],
      "metadata": {
        "id": "EjGK6d2284Cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_split=round(len(df)*0.20)\n",
        "test_split\n",
        "df.iloc[:,1]\n",
        "df_for_training=df[:-test_split]\n",
        "df_for_testing=df[-test_split:]\n",
        "print(df_for_training.shape)\n",
        "print(df_for_testing.shape)\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "df_for_training_scaled = scaler.fit_transform(df_for_training)\n",
        "df_for_testing_scaled=scaler.transform(df_for_testing)\n",
        "df_for_training_scaled[:35]\n",
        "df_for_training_scaled.shape\n",
        "df_for_testing_scaled.shape"
      ],
      "metadata": {
        "id": "xun49Plp8jh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createXY(dataset,n_past):\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "    for i in range(n_past, len(dataset)):\n",
        "            dataX.append(dataset[i - n_past:i, 0:dataset.shape[1]])\n",
        "            dataY.append(dataset[i,0])\n",
        "    return np.array(dataX),np.array(dataY)\n",
        "trainX,trainY=createXY(df_for_training_scaled,30)\n",
        "testX,testY=createXY(df_for_testing_scaled,30)\n",
        "print(trainX.shape)\n",
        "print(trainY.shape)\n",
        "print(testX.shape)\n",
        "print(testY.shape)\n",
        "trainX[1]\n",
        "trainX[0]\n",
        "trainY[0]\n",
        "trainX[1]\n",
        "trainY[1]\n",
        "print(\"trainX Shape-- \",trainX.shape)\n",
        "print(\"trainY Shape-- \",trainY.shape)\n",
        "print(\"testX Shape-- \",testX.shape)\n",
        "print(\"testY Shape-- \",testY.shape)\n",
        "print(\"trainX[0]-- \\n\",trainX[0])\n",
        "print(\"\\ntrainY[0]-- \",trainY[0])"
      ],
      "metadata": {
        "id": "QuaZ7ORt8rCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "import tensorflow.keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "# Generate some random data for demonstration\n",
        "data_size = 1000\n",
        "time_steps = 10\n",
        "future_steps = 3\n",
        "\n",
        "x1 = np.random.rand(data_size, time_steps, 1)\n",
        "x2 = np.random.rand(data_size, time_steps, 1)\n",
        "x3 = np.random.rand(data_size, future_steps, 1)\n",
        "\n",
        "# Concatenate x1 and x2 along the last dimension to form the input_data\n",
        "input_data = np.concatenate([x1, x2], axis=-1)\n",
        "\n",
        "# Generate true labels (y_true) by concatenating the last future_steps of x1 and x2\n",
        "y_true = np.concatenate([x1[:, -future_steps:, :], x2[:, -future_steps:, :]], axis=-1)\n",
        "\n",
        "# Define a custom loss function\n",
        "def custom_loss(y_true, y_pred):\n",
        "    y_pred_expanded = K.expand_dims(y_pred, -1)  # Expanding the last dimension of y_pred\n",
        "    loss_x1 = K.mean(K.square(y_pred_expanded - y_true[:, :, :1]))  # Referencing the first feature from y_true\n",
        "    loss_x2 = K.mean(K.square(y_pred_expanded - y_true[:, :, 1:]))  # Referencing the second feature from y_true\n",
        "    total_loss = loss_x1 + loss_x2\n",
        "    return total_loss\n",
        "\n",
        "# Build the model\n",
        "input_layer = Input(shape=(time_steps, 2))\n",
        "x = LSTM(64, return_sequences=True)(input_layer)\n",
        "x = LSTM(64)(x)\n",
        "output_layer = Dense(future_steps)(x)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model with the custom loss function\n",
        "model.compile(optimizer=Adam(), loss=custom_loss)\n",
        "\n",
        "# Train the model\n",
        "model.fit(input_data, y_true, epochs=3, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQSjyedZUK0E",
        "outputId": "67258f28-a9f0-438a-9d6a-824e2ea0cd2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "32/32 [==============================] - 6s 15ms/step - loss: 0.2257\n",
            "Epoch 2/3\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.1683\n",
            "Epoch 3/3\n",
            "32/32 [==============================] - 1s 19ms/step - loss: 0.1664\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7877a6355090>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# Generate some random data for demonstration\n",
        "data_size = 1000\n",
        "time_steps = 10\n",
        "future_steps = 3\n",
        "\n",
        "x1 = np.random.rand(data_size, time_steps, 1)\n",
        "x2 = np.random.rand(data_size, time_steps, 1)\n",
        "x3 = np.random.rand(data_size, future_steps, 1)\n",
        "\n",
        "# Concatenate x1 and x2 along the last dimension to form the input_data\n",
        "input_data = np.concatenate([x1, x2], axis=-1)\n",
        "\n",
        "# Generate true labels (y_true) by concatenating the last future_steps of x1 and x2\n",
        "y_true = np.concatenate([x1[:, -future_steps:, :], x2[:, -future_steps:, :]], axis=-1)\n",
        "\n",
        "# Split the data into training and testing sets without shuffling\n",
        "input_data_train, input_data_test, y_true_train, y_true_test = train_test_split(input_data, y_true, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Define a custom loss function\n",
        "def custom_loss(y_true, y_pred):\n",
        "    y_pred_expanded = K.expand_dims(y_pred, -1)\n",
        "    loss_x1 = K.mean(K.square(y_pred_expanded - y_true[:, :, :1]))\n",
        "    loss_x2 = K.mean(K.square(y_pred_expanded - y_true[:, :, 1:]))\n",
        "    total_loss = loss_x1 + loss_x2\n",
        "    return total_loss\n",
        "\n",
        "# Build the model\n",
        "input_layer = Input(shape=(time_steps, 2))\n",
        "x = LSTM(64, return_sequences=True)(input_layer)\n",
        "x = LSTM(64)(x)\n",
        "output_layer = Dense(future_steps)(x)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model with the custom loss function\n",
        "model.compile(optimizer=Adam(), loss=custom_loss)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(input_data_train, y_true_train, epochs=3, batch_size=32)\n",
        "\n",
        "# Evaluate the model on the testing data\n",
        "loss = model.evaluate(input_data_test, y_true_test)\n",
        "print(f\"Test Loss: {loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZpVkS4RUkla",
        "outputId": "3120d8e6-900d-4c86-c8e8-29642ef4839d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "25/25 [==============================] - 2s 6ms/step - loss: 0.2583\n",
            "Epoch 2/3\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1624\n",
            "Epoch 3/3\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.1602\n",
            "7/7 [==============================] - 1s 3ms/step - loss: 0.1661\n",
            "Test Loss: 0.16611069440841675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv')\n",
        "\n",
        "# Select relevant columns\n",
        "df_input = df[['Appliances', 'T_out', 'RH_1', 'Visibility']]\n",
        "\n",
        "\"\"\"# Split the data into features (X) and target (y)\n",
        "X = df_input[['T_out', 'RH_1', 'Visibility']].values\n",
        "y = df_input['Appliances'].values\"\"\"\n",
        "\n",
        "\"\"\"# Normalize the features using Min-Max scaling\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\"\"\"\n",
        "X_scaled = X\n",
        "# Define time_steps and future_steps\n",
        "time_steps = 10  # Number of time steps in the input sequence\n",
        "future_steps = 3  # Number of future steps to predict\n",
        "\n",
        "\"\"\"# Create sequences for input data and labels\n",
        "input_data, y_true = [], []\n",
        "for i in range(len(X_scaled) - time_steps - future_steps + 1):\n",
        "    input_data.append(X_scaled[i:i + time_steps])\n",
        "    y_true.append(X_scaled[i + time_steps:i + time_steps + future_steps])\n",
        "\n",
        "input_data = np.array(input_data)\n",
        "y_true = np.array(y_true)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "psW27FK1jbjj",
        "outputId": "a482b643-6e06-4b6b-c7f7-2f6272d0f79f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Create sequences for input data and labels\\ninput_data, y_true = [], []\\nfor i in range(len(X_scaled) - time_steps - future_steps + 1):\\n    input_data.append(X_scaled[i:i + time_steps])\\n    y_true.append(X_scaled[i + time_steps:i + time_steps + future_steps])\\n\\ninput_data = np.array(input_data)\\ny_true = np.array(y_true)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_X1 = np.array(df_input[\"Appliances\"])\n",
        "data_X2 = np.array(df_input[\"T_out\"])\n",
        "data_X3 = np.array(df_input[\"RH_1\"])\n",
        "data_X4 = np.array(df_input[\"Visibility\"])\n",
        "\n",
        "# Step 1 : convert to [rows, columns] structure\n",
        "data_X1= data_X1.reshape( (len(data_X1)), 1)\n",
        "data_X2= data_X2.reshape( (len(data_X2)), 1)\n",
        "data_X3= data_X3.reshape( (len(data_X3)), 1)\n",
        "data_X4= data_X3.reshape( (len(data_X4)), 1)"
      ],
      "metadata": {
        "id": "QjLUYU9AouHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"data_X1.shape\", data_X1.shape)\n",
        "print (\"data_X2.shape\", data_X2.shape)\n",
        "print (\"data_X3.shape\", data_X3.shape)\n",
        "print (\"data_X4.shape\", data_X4.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0XMWvXep0YL",
        "outputId": "548b8199-6c50-4d8f-9fd0-60b731d65976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_X1.shape (19735, 1)\n",
            "data_X2.shape (19735, 1)\n",
            "data_X3.shape (19735, 1)\n",
            "data_X4.shape (19735, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.layers import Input, Dense, Bidirectional, LSTM, RepeatVector, TimeDistributed\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from numpy import array, hstack\n",
        "import numpy as np\n",
        "# Step 3 : horizontally stack columns\n",
        "dataset_stacked = hstack((data_X1,data_X2,data_X3,data_X4))\n",
        "dataset_stacked = np.array(dataset_stacked)\n"
      ],
      "metadata": {
        "id": "Z1lniy6cp1ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"dataset_stacked.shape\", dataset_stacked.shape)\n",
        "print(\"final dataset => \", dataset_stacked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNpmMHWpqHNc",
        "outputId": "b47d7d89-f535-4005-de76-25240489c728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_stacked.shape (19735, 4)\n",
            "final dataset =>  [[ 60.           6.6         47.59666667  47.59666667]\n",
            " [ 60.           6.48333333  46.69333333  46.69333333]\n",
            " [ 50.           6.36666667  46.3         46.3       ]\n",
            " ...\n",
            " [270.          22.46666667  46.59666667  46.59666667]\n",
            " [420.          22.33333333  46.99        46.99      ]\n",
            " [430.          22.2         46.6         46.6       ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset\n",
        "input_timesteps=10\n",
        "input_features = 3\n",
        "output_timesteps = 1\n",
        "output_features = 1\n",
        "\n",
        "data_Y = []\n",
        "data_X = []\n",
        "\n",
        "for i in range(0, ((len(data_X1))-(input_timesteps+output_timesteps))):\n",
        "    print(\"===================\")\n",
        "    tmpY2d = []\n",
        "    for i_row in range(i, (i+ output_timesteps)):\n",
        "        tmpYr = []\n",
        "        print(i_row)\n",
        "        for i_col in range(0, output_features):\n",
        "            tmpYr.append(dataset_stacked[i_row][i_col])\n",
        "        tmpY2d.append(tmpYr)\n",
        "    data_Y.append(tmpY2d)\n",
        "    print(\"-------------------------\")\n",
        "    tmpX2d= []\n",
        "    for j_row in range( (i+ output_timesteps),(i+ output_timesteps + input_timesteps) ):\n",
        "        tmpXr = []\n",
        "        print(j_row)\n",
        "        for j_col in range(0, input_features):\n",
        "            tmpXr.append(dataset_stacked[j_row][j_col])\n",
        "\n",
        "        tmpX2d.append(tmpXr)\n",
        "    data_X.append(tmpX2d)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b_uxRUY-ogzl",
        "outputId": "4bbde311-4c4f-4195-e35e-34b19c20a9d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5091\n",
            "5092\n",
            "5093\n",
            "5094\n",
            "5095\n",
            "===================\n",
            "5086\n",
            "-------------------------\n",
            "5087\n",
            "5088\n",
            "5089\n",
            "5090\n",
            "5091\n",
            "5092\n",
            "5093\n",
            "5094\n",
            "5095\n",
            "5096\n",
            "===================\n",
            "5087\n",
            "-------------------------\n",
            "5088\n",
            "5089\n",
            "5090\n",
            "5091\n",
            "5092\n",
            "5093\n",
            "5094\n",
            "5095\n",
            "5096\n",
            "5097\n",
            "===================\n",
            "5088\n",
            "-------------------------\n",
            "5089\n",
            "5090\n",
            "5091\n",
            "5092\n",
            "5093\n",
            "5094\n",
            "5095\n",
            "5096\n",
            "5097\n",
            "5098\n",
            "===================\n",
            "5089\n",
            "-------------------------\n",
            "5090\n",
            "5091\n",
            "5092\n",
            "5093\n",
            "5094\n",
            "5095\n",
            "5096\n",
            "5097\n",
            "5098\n",
            "5099\n",
            "===================\n",
            "5090\n",
            "-------------------------\n",
            "5091\n",
            "5092\n",
            "5093\n",
            "5094\n",
            "5095\n",
            "5096\n",
            "5097\n",
            "5098\n",
            "5099\n",
            "5100\n",
            "===================\n",
            "5091\n",
            "-------------------------\n",
            "5092\n",
            "5093\n",
            "5094\n",
            "5095\n",
            "5096\n",
            "5097\n",
            "5098\n",
            "5099\n",
            "5100\n",
            "5101\n",
            "===================\n",
            "5092\n",
            "-------------------------\n",
            "5093\n",
            "5094\n",
            "5095\n",
            "5096\n",
            "5097\n",
            "5098\n",
            "5099\n",
            "5100\n",
            "5101\n",
            "5102\n",
            "===================\n",
            "5093\n",
            "-------------------------\n",
            "5094\n",
            "5095\n",
            "5096\n",
            "5097\n",
            "5098\n",
            "5099\n",
            "5100\n",
            "5101\n",
            "5102\n",
            "5103\n",
            "===================\n",
            "5094\n",
            "-------------------------\n",
            "5095\n",
            "5096\n",
            "5097\n",
            "5098\n",
            "5099\n",
            "5100\n",
            "5101\n",
            "5102\n",
            "5103\n",
            "5104\n",
            "===================\n",
            "5095\n",
            "-------------------------\n",
            "5096\n",
            "5097\n",
            "5098\n",
            "5099\n",
            "5100\n",
            "5101\n",
            "5102\n",
            "5103\n",
            "5104\n",
            "5105\n",
            "===================\n",
            "5096\n",
            "-------------------------\n",
            "5097\n",
            "5098\n",
            "5099\n",
            "5100\n",
            "5101\n",
            "5102\n",
            "5103\n",
            "5104\n",
            "5105\n",
            "5106\n",
            "===================\n",
            "5097\n",
            "-------------------------\n",
            "5098\n",
            "5099\n",
            "5100\n",
            "5101\n",
            "5102\n",
            "5103\n",
            "5104\n",
            "5105\n",
            "5106\n",
            "5107\n",
            "===================\n",
            "5098\n",
            "-------------------------\n",
            "5099\n",
            "5100\n",
            "5101\n",
            "5102\n",
            "5103\n",
            "5104\n",
            "5105\n",
            "5106\n",
            "5107\n",
            "5108\n",
            "===================\n",
            "5099\n",
            "-------------------------\n",
            "5100\n",
            "5101\n",
            "5102\n",
            "5103\n",
            "5104\n",
            "5105\n",
            "5106\n",
            "5107\n",
            "5108\n",
            "5109\n",
            "===================\n",
            "5100\n",
            "-------------------------\n",
            "5101\n",
            "5102\n",
            "5103\n",
            "5104\n",
            "5105\n",
            "5106\n",
            "5107\n",
            "5108\n",
            "5109\n",
            "5110\n",
            "===================\n",
            "5101\n",
            "-------------------------\n",
            "5102\n",
            "5103\n",
            "5104\n",
            "5105\n",
            "5106\n",
            "5107\n",
            "5108\n",
            "5109\n",
            "5110\n",
            "5111\n",
            "===================\n",
            "5102\n",
            "-------------------------\n",
            "5103\n",
            "5104\n",
            "5105\n",
            "5106\n",
            "5107\n",
            "5108\n",
            "5109\n",
            "5110\n",
            "5111\n",
            "5112\n",
            "===================\n",
            "5103\n",
            "-------------------------\n",
            "5104\n",
            "5105\n",
            "5106\n",
            "5107\n",
            "5108\n",
            "5109\n",
            "5110\n",
            "5111\n",
            "5112\n",
            "5113\n",
            "===================\n",
            "5104\n",
            "-------------------------\n",
            "5105\n",
            "5106\n",
            "5107\n",
            "5108\n",
            "5109\n",
            "5110\n",
            "5111\n",
            "5112\n",
            "5113\n",
            "5114\n",
            "===================\n",
            "5105\n",
            "-------------------------\n",
            "5106\n",
            "5107\n",
            "5108\n",
            "5109\n",
            "5110\n",
            "5111\n",
            "5112\n",
            "5113\n",
            "5114\n",
            "5115\n",
            "===================\n",
            "5106\n",
            "-------------------------\n",
            "5107\n",
            "5108\n",
            "5109\n",
            "5110\n",
            "5111\n",
            "5112\n",
            "5113\n",
            "5114\n",
            "5115\n",
            "5116\n",
            "===================\n",
            "5107\n",
            "-------------------------\n",
            "5108\n",
            "5109\n",
            "5110\n",
            "5111\n",
            "5112\n",
            "5113\n",
            "5114\n",
            "5115\n",
            "5116\n",
            "5117\n",
            "===================\n",
            "5108\n",
            "-------------------------\n",
            "5109\n",
            "5110\n",
            "5111\n",
            "5112\n",
            "5113\n",
            "5114\n",
            "5115\n",
            "5116\n",
            "5117\n",
            "5118\n",
            "===================\n",
            "5109\n",
            "-------------------------\n",
            "5110\n",
            "5111\n",
            "5112\n",
            "5113\n",
            "5114\n",
            "5115\n",
            "5116\n",
            "5117\n",
            "5118\n",
            "5119\n",
            "===================\n",
            "5110\n",
            "-------------------------\n",
            "5111\n",
            "5112\n",
            "5113\n",
            "5114\n",
            "5115\n",
            "5116\n",
            "5117\n",
            "5118\n",
            "5119\n",
            "5120\n",
            "===================\n",
            "5111\n",
            "-------------------------\n",
            "5112\n",
            "5113\n",
            "5114\n",
            "5115\n",
            "5116\n",
            "5117\n",
            "5118\n",
            "5119\n",
            "5120\n",
            "5121\n",
            "===================\n",
            "5112\n",
            "-------------------------\n",
            "5113\n",
            "5114\n",
            "5115\n",
            "5116\n",
            "5117\n",
            "5118\n",
            "5119\n",
            "5120\n",
            "5121\n",
            "5122\n",
            "===================\n",
            "5113\n",
            "-------------------------\n",
            "5114\n",
            "5115\n",
            "5116\n",
            "5117\n",
            "5118\n",
            "5119\n",
            "5120\n",
            "5121\n",
            "5122\n",
            "5123\n",
            "===================\n",
            "5114\n",
            "-------------------------\n",
            "5115\n",
            "5116\n",
            "5117\n",
            "5118\n",
            "5119\n",
            "5120\n",
            "5121\n",
            "5122\n",
            "5123\n",
            "5124\n",
            "===================\n",
            "5115\n",
            "-------------------------\n",
            "5116\n",
            "5117\n",
            "5118\n",
            "5119\n",
            "5120\n",
            "5121\n",
            "5122\n",
            "5123\n",
            "5124\n",
            "5125\n",
            "===================\n",
            "5116\n",
            "-------------------------\n",
            "5117\n",
            "5118\n",
            "5119\n",
            "5120\n",
            "5121\n",
            "5122\n",
            "5123\n",
            "5124\n",
            "5125\n",
            "5126\n",
            "===================\n",
            "5117\n",
            "-------------------------\n",
            "5118\n",
            "5119\n",
            "5120\n",
            "5121\n",
            "5122\n",
            "5123\n",
            "5124\n",
            "5125\n",
            "5126\n",
            "5127\n",
            "===================\n",
            "5118\n",
            "-------------------------\n",
            "5119\n",
            "5120\n",
            "5121\n",
            "5122\n",
            "5123\n",
            "5124\n",
            "5125\n",
            "5126\n",
            "5127\n",
            "5128\n",
            "===================\n",
            "5119\n",
            "-------------------------\n",
            "5120\n",
            "5121\n",
            "5122\n",
            "5123\n",
            "5124\n",
            "5125\n",
            "5126\n",
            "5127\n",
            "5128\n",
            "5129\n",
            "===================\n",
            "5120\n",
            "-------------------------\n",
            "5121\n",
            "5122\n",
            "5123\n",
            "5124\n",
            "5125\n",
            "5126\n",
            "5127\n",
            "5128\n",
            "5129\n",
            "5130\n",
            "===================\n",
            "5121\n",
            "-------------------------\n",
            "5122\n",
            "5123\n",
            "5124\n",
            "5125\n",
            "5126\n",
            "5127\n",
            "5128\n",
            "5129\n",
            "5130\n",
            "5131\n",
            "===================\n",
            "5122\n",
            "-------------------------\n",
            "5123\n",
            "5124\n",
            "5125\n",
            "5126\n",
            "5127\n",
            "5128\n",
            "5129\n",
            "5130\n",
            "5131\n",
            "5132\n",
            "===================\n",
            "5123\n",
            "-------------------------\n",
            "5124\n",
            "5125\n",
            "5126\n",
            "5127\n",
            "5128\n",
            "5129\n",
            "5130\n",
            "5131\n",
            "5132\n",
            "5133\n",
            "===================\n",
            "5124\n",
            "-------------------------\n",
            "5125\n",
            "5126\n",
            "5127\n",
            "5128\n",
            "5129\n",
            "5130\n",
            "5131\n",
            "5132\n",
            "5133\n",
            "5134\n",
            "===================\n",
            "5125\n",
            "-------------------------\n",
            "5126\n",
            "5127\n",
            "5128\n",
            "5129\n",
            "5130\n",
            "5131\n",
            "5132\n",
            "5133\n",
            "5134\n",
            "5135\n",
            "===================\n",
            "5126\n",
            "-------------------------\n",
            "5127\n",
            "5128\n",
            "5129\n",
            "5130\n",
            "5131\n",
            "5132\n",
            "5133\n",
            "5134\n",
            "5135\n",
            "5136\n",
            "===================\n",
            "5127\n",
            "-------------------------\n",
            "5128\n",
            "5129\n",
            "5130\n",
            "5131\n",
            "5132\n",
            "5133\n",
            "5134\n",
            "5135\n",
            "5136\n",
            "5137\n",
            "===================\n",
            "5128\n",
            "-------------------------\n",
            "5129\n",
            "5130\n",
            "5131\n",
            "5132\n",
            "5133\n",
            "5134\n",
            "5135\n",
            "5136\n",
            "5137\n",
            "5138\n",
            "===================\n",
            "5129\n",
            "-------------------------\n",
            "5130\n",
            "5131\n",
            "5132\n",
            "5133\n",
            "5134\n",
            "5135\n",
            "5136\n",
            "5137\n",
            "5138\n",
            "5139\n",
            "===================\n",
            "5130\n",
            "-------------------------\n",
            "5131\n",
            "5132\n",
            "5133\n",
            "5134\n",
            "5135\n",
            "5136\n",
            "5137\n",
            "5138\n",
            "5139\n",
            "5140\n",
            "===================\n",
            "5131\n",
            "-------------------------\n",
            "5132\n",
            "5133\n",
            "5134\n",
            "5135\n",
            "5136\n",
            "5137\n",
            "5138\n",
            "5139\n",
            "5140\n",
            "5141\n",
            "===================\n",
            "5132\n",
            "-------------------------\n",
            "5133\n",
            "5134\n",
            "5135\n",
            "5136\n",
            "5137\n",
            "5138\n",
            "5139\n",
            "5140\n",
            "5141\n",
            "5142\n",
            "===================\n",
            "5133\n",
            "-------------------------\n",
            "5134\n",
            "5135\n",
            "5136\n",
            "5137\n",
            "5138\n",
            "5139\n",
            "5140\n",
            "5141\n",
            "5142\n",
            "5143\n",
            "===================\n",
            "5134\n",
            "-------------------------\n",
            "5135\n",
            "5136\n",
            "5137\n",
            "5138\n",
            "5139\n",
            "5140\n",
            "5141\n",
            "5142\n",
            "5143\n",
            "5144\n",
            "===================\n",
            "5135\n",
            "-------------------------\n",
            "5136\n",
            "5137\n",
            "5138\n",
            "5139\n",
            "5140\n",
            "5141\n",
            "5142\n",
            "5143\n",
            "5144\n",
            "5145\n",
            "===================\n",
            "5136\n",
            "-------------------------\n",
            "5137\n",
            "5138\n",
            "5139\n",
            "5140\n",
            "5141\n",
            "5142\n",
            "5143\n",
            "5144\n",
            "5145\n",
            "5146\n",
            "===================\n",
            "5137\n",
            "-------------------------\n",
            "5138\n",
            "5139\n",
            "5140\n",
            "5141\n",
            "5142\n",
            "5143\n",
            "5144\n",
            "5145\n",
            "5146\n",
            "5147\n",
            "===================\n",
            "5138\n",
            "-------------------------\n",
            "5139\n",
            "5140\n",
            "5141\n",
            "5142\n",
            "5143\n",
            "5144\n",
            "5145\n",
            "5146\n",
            "5147\n",
            "5148\n",
            "===================\n",
            "5139\n",
            "-------------------------\n",
            "5140\n",
            "5141\n",
            "5142\n",
            "5143\n",
            "5144\n",
            "5145\n",
            "5146\n",
            "5147\n",
            "5148\n",
            "5149\n",
            "===================\n",
            "5140\n",
            "-------------------------\n",
            "5141\n",
            "5142\n",
            "5143\n",
            "5144\n",
            "5145\n",
            "5146\n",
            "5147\n",
            "5148\n",
            "5149\n",
            "5150\n",
            "===================\n",
            "5141\n",
            "-------------------------\n",
            "5142\n",
            "5143\n",
            "5144\n",
            "5145\n",
            "5146\n",
            "5147\n",
            "5148\n",
            "5149\n",
            "5150\n",
            "5151\n",
            "===================\n",
            "5142\n",
            "-------------------------\n",
            "5143\n",
            "5144\n",
            "5145\n",
            "5146\n",
            "5147\n",
            "5148\n",
            "5149\n",
            "5150\n",
            "5151\n",
            "5152\n",
            "===================\n",
            "5143\n",
            "-------------------------\n",
            "5144\n",
            "5145\n",
            "5146\n",
            "5147\n",
            "5148\n",
            "5149\n",
            "5150\n",
            "5151\n",
            "5152\n",
            "5153\n",
            "===================\n",
            "5144\n",
            "-------------------------\n",
            "5145\n",
            "5146\n",
            "5147\n",
            "5148\n",
            "5149\n",
            "5150\n",
            "5151\n",
            "5152\n",
            "5153\n",
            "5154\n",
            "===================\n",
            "5145\n",
            "-------------------------\n",
            "5146\n",
            "5147\n",
            "5148\n",
            "5149\n",
            "5150\n",
            "5151\n",
            "5152\n",
            "5153\n",
            "5154\n",
            "5155\n",
            "===================\n",
            "5146\n",
            "-------------------------\n",
            "5147\n",
            "5148\n",
            "5149\n",
            "5150\n",
            "5151\n",
            "5152\n",
            "5153\n",
            "5154\n",
            "5155\n",
            "5156\n",
            "===================\n",
            "5147\n",
            "-------------------------\n",
            "5148\n",
            "5149\n",
            "5150\n",
            "5151\n",
            "5152\n",
            "5153\n",
            "5154\n",
            "5155\n",
            "5156\n",
            "5157\n",
            "===================\n",
            "5148\n",
            "-------------------------\n",
            "5149\n",
            "5150\n",
            "5151\n",
            "5152\n",
            "5153\n",
            "5154\n",
            "5155\n",
            "5156\n",
            "5157\n",
            "5158\n",
            "===================\n",
            "5149\n",
            "-------------------------\n",
            "5150\n",
            "5151\n",
            "5152\n",
            "5153\n",
            "5154\n",
            "5155\n",
            "5156\n",
            "5157\n",
            "5158\n",
            "5159\n",
            "===================\n",
            "5150\n",
            "-------------------------\n",
            "5151\n",
            "5152\n",
            "5153\n",
            "5154\n",
            "5155\n",
            "5156\n",
            "5157\n",
            "5158\n",
            "5159\n",
            "5160\n",
            "===================\n",
            "5151\n",
            "-------------------------\n",
            "5152\n",
            "5153\n",
            "5154\n",
            "5155\n",
            "5156\n",
            "5157\n",
            "5158\n",
            "5159\n",
            "5160\n",
            "5161\n",
            "===================\n",
            "5152\n",
            "-------------------------\n",
            "5153\n",
            "5154\n",
            "5155\n",
            "5156\n",
            "5157\n",
            "5158\n",
            "5159\n",
            "5160\n",
            "5161\n",
            "5162\n",
            "===================\n",
            "5153\n",
            "-------------------------\n",
            "5154\n",
            "5155\n",
            "5156\n",
            "5157\n",
            "5158\n",
            "5159\n",
            "5160\n",
            "5161\n",
            "5162\n",
            "5163\n",
            "===================\n",
            "5154\n",
            "-------------------------\n",
            "5155\n",
            "5156\n",
            "5157\n",
            "5158\n",
            "5159\n",
            "5160\n",
            "5161\n",
            "5162\n",
            "5163\n",
            "5164\n",
            "===================\n",
            "5155\n",
            "-------------------------\n",
            "5156\n",
            "5157\n",
            "5158\n",
            "5159\n",
            "5160\n",
            "5161\n",
            "5162\n",
            "5163\n",
            "5164\n",
            "5165\n",
            "===================\n",
            "5156\n",
            "-------------------------\n",
            "5157\n",
            "5158\n",
            "5159\n",
            "5160\n",
            "5161\n",
            "5162\n",
            "5163\n",
            "5164\n",
            "5165\n",
            "5166\n",
            "===================\n",
            "5157\n",
            "-------------------------\n",
            "5158\n",
            "5159\n",
            "5160\n",
            "5161\n",
            "5162\n",
            "5163\n",
            "5164\n",
            "5165\n",
            "5166\n",
            "5167\n",
            "===================\n",
            "5158\n",
            "-------------------------\n",
            "5159\n",
            "5160\n",
            "5161\n",
            "5162\n",
            "5163\n",
            "5164\n",
            "5165\n",
            "5166\n",
            "5167\n",
            "5168\n",
            "===================\n",
            "5159\n",
            "-------------------------\n",
            "5160\n",
            "5161\n",
            "5162\n",
            "5163\n",
            "5164\n",
            "5165\n",
            "5166\n",
            "5167\n",
            "5168\n",
            "5169\n",
            "===================\n",
            "5160\n",
            "-------------------------\n",
            "5161\n",
            "5162\n",
            "5163\n",
            "5164\n",
            "5165\n",
            "5166\n",
            "5167\n",
            "5168\n",
            "5169\n",
            "5170\n",
            "===================\n",
            "5161\n",
            "-------------------------\n",
            "5162\n",
            "5163\n",
            "5164\n",
            "5165\n",
            "5166\n",
            "5167\n",
            "5168\n",
            "5169\n",
            "5170\n",
            "5171\n",
            "===================\n",
            "5162\n",
            "-------------------------\n",
            "5163\n",
            "5164\n",
            "5165\n",
            "5166\n",
            "5167\n",
            "5168\n",
            "5169\n",
            "5170\n",
            "5171\n",
            "5172\n",
            "===================\n",
            "5163\n",
            "-------------------------\n",
            "5164\n",
            "5165\n",
            "5166\n",
            "5167\n",
            "5168\n",
            "5169\n",
            "5170\n",
            "5171\n",
            "5172\n",
            "5173\n",
            "===================\n",
            "5164\n",
            "-------------------------\n",
            "5165\n",
            "5166\n",
            "5167\n",
            "5168\n",
            "5169\n",
            "5170\n",
            "5171\n",
            "5172\n",
            "5173\n",
            "5174\n",
            "===================\n",
            "5165\n",
            "-------------------------\n",
            "5166\n",
            "5167\n",
            "5168\n",
            "5169\n",
            "5170\n",
            "5171\n",
            "5172\n",
            "5173\n",
            "5174\n",
            "5175\n",
            "===================\n",
            "5166\n",
            "-------------------------\n",
            "5167\n",
            "5168\n",
            "5169\n",
            "5170\n",
            "5171\n",
            "5172\n",
            "5173\n",
            "5174\n",
            "5175\n",
            "5176\n",
            "===================\n",
            "5167\n",
            "-------------------------\n",
            "5168\n",
            "5169\n",
            "5170\n",
            "5171\n",
            "5172\n",
            "5173\n",
            "5174\n",
            "5175\n",
            "5176\n",
            "5177\n",
            "===================\n",
            "5168\n",
            "-------------------------\n",
            "5169\n",
            "5170\n",
            "5171\n",
            "5172\n",
            "5173\n",
            "5174\n",
            "5175\n",
            "5176\n",
            "5177\n",
            "5178\n",
            "===================\n",
            "5169\n",
            "-------------------------\n",
            "5170\n",
            "5171\n",
            "5172\n",
            "5173\n",
            "5174\n",
            "5175\n",
            "5176\n",
            "5177\n",
            "5178\n",
            "5179\n",
            "===================\n",
            "5170\n",
            "-------------------------\n",
            "5171\n",
            "5172\n",
            "5173\n",
            "5174\n",
            "5175\n",
            "5176\n",
            "5177\n",
            "5178\n",
            "5179\n",
            "5180\n",
            "===================\n",
            "5171\n",
            "-------------------------\n",
            "5172\n",
            "5173\n",
            "5174\n",
            "5175\n",
            "5176\n",
            "5177\n",
            "5178\n",
            "5179\n",
            "5180\n",
            "5181\n",
            "===================\n",
            "5172\n",
            "-------------------------\n",
            "5173\n",
            "5174\n",
            "5175\n",
            "5176\n",
            "5177\n",
            "5178\n",
            "5179\n",
            "5180\n",
            "5181\n",
            "5182\n",
            "===================\n",
            "5173\n",
            "-------------------------\n",
            "5174\n",
            "5175\n",
            "5176\n",
            "5177\n",
            "5178\n",
            "5179\n",
            "5180\n",
            "5181\n",
            "5182\n",
            "5183\n",
            "===================\n",
            "5174\n",
            "-------------------------\n",
            "5175\n",
            "5176\n",
            "5177\n",
            "5178\n",
            "5179\n",
            "5180\n",
            "5181\n",
            "5182\n",
            "5183\n",
            "5184\n",
            "===================\n",
            "5175\n",
            "-------------------------\n",
            "5176\n",
            "5177\n",
            "5178\n",
            "5179\n",
            "5180\n",
            "5181\n",
            "5182\n",
            "5183\n",
            "5184\n",
            "5185\n",
            "===================\n",
            "5176\n",
            "-------------------------\n",
            "5177\n",
            "5178\n",
            "5179\n",
            "5180\n",
            "5181\n",
            "5182\n",
            "5183\n",
            "5184\n",
            "5185\n",
            "5186\n",
            "===================\n",
            "5177\n",
            "-------------------------\n",
            "5178\n",
            "5179\n",
            "5180\n",
            "5181\n",
            "5182\n",
            "5183\n",
            "5184\n",
            "5185\n",
            "5186\n",
            "5187\n",
            "===================\n",
            "5178\n",
            "-------------------------\n",
            "5179\n",
            "5180\n",
            "5181\n",
            "5182\n",
            "5183\n",
            "5184\n",
            "5185\n",
            "5186\n",
            "5187\n",
            "5188\n",
            "===================\n",
            "5179\n",
            "-------------------------\n",
            "5180\n",
            "5181\n",
            "5182\n",
            "5183\n",
            "5184\n",
            "5185\n",
            "5186\n",
            "5187\n",
            "5188\n",
            "5189\n",
            "===================\n",
            "5180\n",
            "-------------------------\n",
            "5181\n",
            "5182\n",
            "5183\n",
            "5184\n",
            "5185\n",
            "5186\n",
            "5187\n",
            "5188\n",
            "5189\n",
            "5190\n",
            "===================\n",
            "5181\n",
            "-------------------------\n",
            "5182\n",
            "5183\n",
            "5184\n",
            "5185\n",
            "5186\n",
            "5187\n",
            "5188\n",
            "5189\n",
            "5190\n",
            "5191\n",
            "===================\n",
            "5182\n",
            "-------------------------\n",
            "5183\n",
            "5184\n",
            "5185\n",
            "5186\n",
            "5187\n",
            "5188\n",
            "5189\n",
            "5190\n",
            "5191\n",
            "5192\n",
            "===================\n",
            "5183\n",
            "-------------------------\n",
            "5184\n",
            "5185\n",
            "5186\n",
            "5187\n",
            "5188\n",
            "5189\n",
            "5190\n",
            "5191\n",
            "5192\n",
            "5193\n",
            "===================\n",
            "5184\n",
            "-------------------------\n",
            "5185\n",
            "5186\n",
            "5187\n",
            "5188\n",
            "5189\n",
            "5190\n",
            "5191\n",
            "5192\n",
            "5193\n",
            "5194\n",
            "===================\n",
            "5185\n",
            "-------------------------\n",
            "5186\n",
            "5187\n",
            "5188\n",
            "5189\n",
            "5190\n",
            "5191\n",
            "5192\n",
            "5193\n",
            "5194\n",
            "5195\n",
            "===================\n",
            "5186\n",
            "-------------------------\n",
            "5187\n",
            "5188\n",
            "5189\n",
            "5190\n",
            "5191\n",
            "5192\n",
            "5193\n",
            "5194\n",
            "5195\n",
            "5196\n",
            "===================\n",
            "5187\n",
            "-------------------------\n",
            "5188\n",
            "5189\n",
            "5190\n",
            "5191\n",
            "5192\n",
            "5193\n",
            "5194\n",
            "5195\n",
            "5196\n",
            "5197\n",
            "===================\n",
            "5188\n",
            "-------------------------\n",
            "5189\n",
            "5190\n",
            "5191\n",
            "5192\n",
            "5193\n",
            "5194\n",
            "5195\n",
            "5196\n",
            "5197\n",
            "5198\n",
            "===================\n",
            "5189\n",
            "-------------------------\n",
            "5190\n",
            "5191\n",
            "5192\n",
            "5193\n",
            "5194\n",
            "5195\n",
            "5196\n",
            "5197\n",
            "5198\n",
            "5199\n",
            "===================\n",
            "5190\n",
            "-------------------------\n",
            "5191\n",
            "5192\n",
            "5193\n",
            "5194\n",
            "5195\n",
            "5196\n",
            "5197\n",
            "5198\n",
            "5199\n",
            "5200\n",
            "===================\n",
            "5191\n",
            "-------------------------\n",
            "5192\n",
            "5193\n",
            "5194\n",
            "5195\n",
            "5196\n",
            "5197\n",
            "5198\n",
            "5199\n",
            "5200\n",
            "5201\n",
            "===================\n",
            "5192\n",
            "-------------------------\n",
            "5193\n",
            "5194\n",
            "5195\n",
            "5196\n",
            "5197\n",
            "5198\n",
            "5199\n",
            "5200\n",
            "5201\n",
            "5202\n",
            "===================\n",
            "5193\n",
            "-------------------------\n",
            "5194\n",
            "5195\n",
            "5196\n",
            "5197\n",
            "5198\n",
            "5199\n",
            "5200\n",
            "5201\n",
            "5202\n",
            "5203\n",
            "===================\n",
            "5194\n",
            "-------------------------\n",
            "5195\n",
            "5196\n",
            "5197\n",
            "5198\n",
            "5199\n",
            "5200\n",
            "5201\n",
            "5202\n",
            "5203\n",
            "5204\n",
            "===================\n",
            "5195\n",
            "-------------------------\n",
            "5196\n",
            "5197\n",
            "5198\n",
            "5199\n",
            "5200\n",
            "5201\n",
            "5202\n",
            "5203\n",
            "5204\n",
            "5205\n",
            "===================\n",
            "5196\n",
            "-------------------------\n",
            "5197\n",
            "5198\n",
            "5199\n",
            "5200\n",
            "5201\n",
            "5202\n",
            "5203\n",
            "5204\n",
            "5205\n",
            "5206\n",
            "===================\n",
            "5197\n",
            "-------------------------\n",
            "5198\n",
            "5199\n",
            "5200\n",
            "5201\n",
            "5202\n",
            "5203\n",
            "5204\n",
            "5205\n",
            "5206\n",
            "5207\n",
            "===================\n",
            "5198\n",
            "-------------------------\n",
            "5199\n",
            "5200\n",
            "5201\n",
            "5202\n",
            "5203\n",
            "5204\n",
            "5205\n",
            "5206\n",
            "5207\n",
            "5208\n",
            "===================\n",
            "5199\n",
            "-------------------------\n",
            "5200\n",
            "5201\n",
            "5202\n",
            "5203\n",
            "5204\n",
            "5205\n",
            "5206\n",
            "5207\n",
            "5208\n",
            "5209\n",
            "===================\n",
            "5200\n",
            "-------------------------\n",
            "5201\n",
            "5202\n",
            "5203\n",
            "5204\n",
            "5205\n",
            "5206\n",
            "5207\n",
            "5208\n",
            "5209\n",
            "5210\n",
            "===================\n",
            "5201\n",
            "-------------------------\n",
            "5202\n",
            "5203\n",
            "5204\n",
            "5205\n",
            "5206\n",
            "5207\n",
            "5208\n",
            "5209\n",
            "5210\n",
            "5211\n",
            "===================\n",
            "5202\n",
            "-------------------------\n",
            "5203\n",
            "5204\n",
            "5205\n",
            "5206\n",
            "5207\n",
            "5208\n",
            "5209\n",
            "5210\n",
            "5211\n",
            "5212\n",
            "===================\n",
            "5203\n",
            "-------------------------\n",
            "5204\n",
            "5205\n",
            "5206\n",
            "5207\n",
            "5208\n",
            "5209\n",
            "5210\n",
            "5211\n",
            "5212\n",
            "5213\n",
            "===================\n",
            "5204\n",
            "-------------------------\n",
            "5205\n",
            "5206\n",
            "5207\n",
            "5208\n",
            "5209\n",
            "5210\n",
            "5211\n",
            "5212\n",
            "5213\n",
            "5214\n",
            "===================\n",
            "5205\n",
            "-------------------------\n",
            "5206\n",
            "5207\n",
            "5208\n",
            "5209\n",
            "5210\n",
            "5211\n",
            "5212\n",
            "5213\n",
            "5214\n",
            "5215\n",
            "===================\n",
            "5206\n",
            "-------------------------\n",
            "5207\n",
            "5208\n",
            "5209\n",
            "5210\n",
            "5211\n",
            "5212\n",
            "5213\n",
            "5214\n",
            "5215\n",
            "5216\n",
            "===================\n",
            "5207\n",
            "-------------------------\n",
            "5208\n",
            "5209\n",
            "5210\n",
            "5211\n",
            "5212\n",
            "5213\n",
            "5214\n",
            "5215\n",
            "5216\n",
            "5217\n",
            "===================\n",
            "5208\n",
            "-------------------------\n",
            "5209\n",
            "5210\n",
            "5211\n",
            "5212\n",
            "5213\n",
            "5214\n",
            "5215\n",
            "5216\n",
            "5217\n",
            "5218\n",
            "===================\n",
            "5209\n",
            "-------------------------\n",
            "5210\n",
            "5211\n",
            "5212\n",
            "5213\n",
            "5214\n",
            "5215\n",
            "5216\n",
            "5217\n",
            "5218\n",
            "5219\n",
            "===================\n",
            "5210\n",
            "-------------------------\n",
            "5211\n",
            "5212\n",
            "5213\n",
            "5214\n",
            "5215\n",
            "5216\n",
            "5217\n",
            "5218\n",
            "5219\n",
            "5220\n",
            "===================\n",
            "5211\n",
            "-------------------------\n",
            "5212\n",
            "5213\n",
            "5214\n",
            "5215\n",
            "5216\n",
            "5217\n",
            "5218\n",
            "5219\n",
            "5220\n",
            "5221\n",
            "===================\n",
            "5212\n",
            "-------------------------\n",
            "5213\n",
            "5214\n",
            "5215\n",
            "5216\n",
            "5217\n",
            "5218\n",
            "5219\n",
            "5220\n",
            "5221\n",
            "5222\n",
            "===================\n",
            "5213\n",
            "-------------------------\n",
            "5214\n",
            "5215\n",
            "5216\n",
            "5217\n",
            "5218\n",
            "5219\n",
            "5220\n",
            "5221\n",
            "5222\n",
            "5223\n",
            "===================\n",
            "5214\n",
            "-------------------------\n",
            "5215\n",
            "5216\n",
            "5217\n",
            "5218\n",
            "5219\n",
            "5220\n",
            "5221\n",
            "5222\n",
            "5223\n",
            "5224\n",
            "===================\n",
            "5215\n",
            "-------------------------\n",
            "5216\n",
            "5217\n",
            "5218\n",
            "5219\n",
            "5220\n",
            "5221\n",
            "5222\n",
            "5223\n",
            "5224\n",
            "5225\n",
            "===================\n",
            "5216\n",
            "-------------------------\n",
            "5217\n",
            "5218\n",
            "5219\n",
            "5220\n",
            "5221\n",
            "5222\n",
            "5223\n",
            "5224\n",
            "5225\n",
            "5226\n",
            "===================\n",
            "5217\n",
            "-------------------------\n",
            "5218\n",
            "5219\n",
            "5220\n",
            "5221\n",
            "5222\n",
            "5223\n",
            "5224\n",
            "5225\n",
            "5226\n",
            "5227\n",
            "===================\n",
            "5218\n",
            "-------------------------\n",
            "5219\n",
            "5220\n",
            "5221\n",
            "5222\n",
            "5223\n",
            "5224\n",
            "5225\n",
            "5226\n",
            "5227\n",
            "5228\n",
            "===================\n",
            "5219\n",
            "-------------------------\n",
            "5220\n",
            "5221\n",
            "5222\n",
            "5223\n",
            "5224\n",
            "5225\n",
            "5226\n",
            "5227\n",
            "5228\n",
            "5229\n",
            "===================\n",
            "5220\n",
            "-------------------------\n",
            "5221\n",
            "5222\n",
            "5223\n",
            "5224\n",
            "5225\n",
            "5226\n",
            "5227\n",
            "5228\n",
            "5229\n",
            "5230\n",
            "===================\n",
            "5221\n",
            "-------------------------\n",
            "5222\n",
            "5223\n",
            "5224\n",
            "5225\n",
            "5226\n",
            "5227\n",
            "5228\n",
            "5229\n",
            "5230\n",
            "5231\n",
            "===================\n",
            "5222\n",
            "-------------------------\n",
            "5223\n",
            "5224\n",
            "5225\n",
            "5226\n",
            "5227\n",
            "5228\n",
            "5229\n",
            "5230\n",
            "5231\n",
            "5232\n",
            "===================\n",
            "5223\n",
            "-------------------------\n",
            "5224\n",
            "5225\n",
            "5226\n",
            "5227\n",
            "5228\n",
            "5229\n",
            "5230\n",
            "5231\n",
            "5232\n",
            "5233\n",
            "===================\n",
            "5224\n",
            "-------------------------\n",
            "5225\n",
            "5226\n",
            "5227\n",
            "5228\n",
            "5229\n",
            "5230\n",
            "5231\n",
            "5232\n",
            "5233\n",
            "5234\n",
            "===================\n",
            "5225\n",
            "-------------------------\n",
            "5226\n",
            "5227\n",
            "5228\n",
            "5229\n",
            "5230\n",
            "5231\n",
            "5232\n",
            "5233\n",
            "5234\n",
            "5235\n",
            "===================\n",
            "5226\n",
            "-------------------------\n",
            "5227\n",
            "5228\n",
            "5229\n",
            "5230\n",
            "5231\n",
            "5232\n",
            "5233\n",
            "5234\n",
            "5235\n",
            "5236\n",
            "===================\n",
            "5227\n",
            "-------------------------\n",
            "5228\n",
            "5229\n",
            "5230\n",
            "5231\n",
            "5232\n",
            "5233\n",
            "5234\n",
            "5235\n",
            "5236\n",
            "5237\n",
            "===================\n",
            "5228\n",
            "-------------------------\n",
            "5229\n",
            "5230\n",
            "5231\n",
            "5232\n",
            "5233\n",
            "5234\n",
            "5235\n",
            "5236\n",
            "5237\n",
            "5238\n",
            "===================\n",
            "5229\n",
            "-------------------------\n",
            "5230\n",
            "5231\n",
            "5232\n",
            "5233\n",
            "5234\n",
            "5235\n",
            "5236\n",
            "5237\n",
            "5238\n",
            "5239\n",
            "===================\n",
            "5230\n",
            "-------------------------\n",
            "5231\n",
            "5232\n",
            "5233\n",
            "5234\n",
            "5235\n",
            "5236\n",
            "5237\n",
            "5238\n",
            "5239\n",
            "5240\n",
            "===================\n",
            "5231\n",
            "-------------------------\n",
            "5232\n",
            "5233\n",
            "5234\n",
            "5235\n",
            "5236\n",
            "5237\n",
            "5238\n",
            "5239\n",
            "5240\n",
            "5241\n",
            "===================\n",
            "5232\n",
            "-------------------------\n",
            "5233\n",
            "5234\n",
            "5235\n",
            "5236\n",
            "5237\n",
            "5238\n",
            "5239\n",
            "5240\n",
            "5241\n",
            "5242\n",
            "===================\n",
            "5233\n",
            "-------------------------\n",
            "5234\n",
            "5235\n",
            "5236\n",
            "5237\n",
            "5238\n",
            "5239\n",
            "5240\n",
            "5241\n",
            "5242\n",
            "5243\n",
            "===================\n",
            "5234\n",
            "-------------------------\n",
            "5235\n",
            "5236\n",
            "5237\n",
            "5238\n",
            "5239\n",
            "5240\n",
            "5241\n",
            "5242\n",
            "5243\n",
            "5244\n",
            "===================\n",
            "5235\n",
            "-------------------------\n",
            "5236\n",
            "5237\n",
            "5238\n",
            "5239\n",
            "5240\n",
            "5241\n",
            "5242\n",
            "5243\n",
            "5244\n",
            "5245\n",
            "===================\n",
            "5236\n",
            "-------------------------\n",
            "5237\n",
            "5238\n",
            "5239\n",
            "5240\n",
            "5241\n",
            "5242\n",
            "5243\n",
            "5244\n",
            "5245\n",
            "5246\n",
            "===================\n",
            "5237\n",
            "-------------------------\n",
            "5238\n",
            "5239\n",
            "5240\n",
            "5241\n",
            "5242\n",
            "5243\n",
            "5244\n",
            "5245\n",
            "5246\n",
            "5247\n",
            "===================\n",
            "5238\n",
            "-------------------------\n",
            "5239\n",
            "5240\n",
            "5241\n",
            "5242\n",
            "5243\n",
            "5244\n",
            "5245\n",
            "5246\n",
            "5247\n",
            "5248\n",
            "===================\n",
            "5239\n",
            "-------------------------\n",
            "5240\n",
            "5241\n",
            "5242\n",
            "5243\n",
            "5244\n",
            "5245\n",
            "5246\n",
            "5247\n",
            "5248\n",
            "5249\n",
            "===================\n",
            "5240\n",
            "-------------------------\n",
            "5241\n",
            "5242\n",
            "5243\n",
            "5244\n",
            "5245\n",
            "5246\n",
            "5247\n",
            "5248\n",
            "5249\n",
            "5250\n",
            "===================\n",
            "5241\n",
            "-------------------------\n",
            "5242\n",
            "5243\n",
            "5244\n",
            "5245\n",
            "5246\n",
            "5247\n",
            "5248\n",
            "5249\n",
            "5250\n",
            "5251\n",
            "===================\n",
            "5242\n",
            "-------------------------\n",
            "5243\n",
            "5244\n",
            "5245\n",
            "5246\n",
            "5247\n",
            "5248\n",
            "5249\n",
            "5250\n",
            "5251\n",
            "5252\n",
            "===================\n",
            "5243\n",
            "-------------------------\n",
            "5244\n",
            "5245\n",
            "5246\n",
            "5247\n",
            "5248\n",
            "5249\n",
            "5250\n",
            "5251\n",
            "5252\n",
            "5253\n",
            "===================\n",
            "5244\n",
            "-------------------------\n",
            "5245\n",
            "5246\n",
            "5247\n",
            "5248\n",
            "5249\n",
            "5250\n",
            "5251\n",
            "5252\n",
            "5253\n",
            "5254\n",
            "===================\n",
            "5245\n",
            "-------------------------\n",
            "5246\n",
            "5247\n",
            "5248\n",
            "5249\n",
            "5250\n",
            "5251\n",
            "5252\n",
            "5253\n",
            "5254\n",
            "5255\n",
            "===================\n",
            "5246\n",
            "-------------------------\n",
            "5247\n",
            "5248\n",
            "5249\n",
            "5250\n",
            "5251\n",
            "5252\n",
            "5253\n",
            "5254\n",
            "5255\n",
            "5256\n",
            "===================\n",
            "5247\n",
            "-------------------------\n",
            "5248\n",
            "5249\n",
            "5250\n",
            "5251\n",
            "5252\n",
            "5253\n",
            "5254\n",
            "5255\n",
            "5256\n",
            "5257\n",
            "===================\n",
            "5248\n",
            "-------------------------\n",
            "5249\n",
            "5250\n",
            "5251\n",
            "5252\n",
            "5253\n",
            "5254\n",
            "5255\n",
            "5256\n",
            "5257\n",
            "5258\n",
            "===================\n",
            "5249\n",
            "-------------------------\n",
            "5250\n",
            "5251\n",
            "5252\n",
            "5253\n",
            "5254\n",
            "5255\n",
            "5256\n",
            "5257\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-403b3e5bb4f3>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj_row\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0moutput_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0moutput_timesteps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_timesteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtmpXr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj_col\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mtmpXr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_stacked\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj_row\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;31m# mp.Pool cannot be trusted to flush promptly (or ever),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    616\u001b[0m                 )\n\u001b[1;32m    617\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     def send_multipart(\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_X = np.array(data_X)\n",
        "data_Y = np.array(data_Y)\n",
        "print(\"shape of input data\", data_X.shape)\n",
        "print(\"input data => \", data_X)\n",
        "print(\"shape of output data\", data_Y.shape)\n",
        "print(\"output data => \", data_Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgGB2RWTqqTb",
        "outputId": "94d303fe-1679-46f1-a4ff-a1b9f9b31c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of input data (19724, 10, 3)\n",
            "input data =>  [[[ 60.           6.48333333  46.69333333]\n",
            "  [ 50.           6.36666667  46.3       ]\n",
            "  [ 50.           6.25        46.06666667]\n",
            "  ...\n",
            "  [ 60.           5.93333333  45.5975    ]\n",
            "  [ 70.           5.95        46.09      ]\n",
            "  [230.           5.96666667  45.86333333]]\n",
            "\n",
            " [[ 50.           6.36666667  46.3       ]\n",
            "  [ 50.           6.25        46.06666667]\n",
            "  [ 60.           6.13333333  46.33333333]\n",
            "  ...\n",
            "  [ 70.           5.95        46.09      ]\n",
            "  [230.           5.96666667  45.86333333]\n",
            "  [580.           5.98333333  46.39666667]]\n",
            "\n",
            " [[ 50.           6.25        46.06666667]\n",
            "  [ 60.           6.13333333  46.33333333]\n",
            "  [ 50.           6.01666667  46.02666667]\n",
            "  ...\n",
            "  [230.           5.96666667  45.86333333]\n",
            "  [580.           5.98333333  46.39666667]\n",
            "  [430.           6.          48.        ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[100.          22.8         45.73      ]\n",
            "  [ 90.          22.83333333  45.79      ]\n",
            "  [100.          22.86666667  45.93333333]\n",
            "  ...\n",
            "  [ 90.          22.86666667  46.86      ]\n",
            "  [100.          22.73333333  46.56      ]\n",
            "  [ 90.          22.6         46.5       ]]\n",
            "\n",
            " [[ 90.          22.83333333  45.79      ]\n",
            "  [100.          22.86666667  45.93333333]\n",
            "  [220.          22.9         46.06      ]\n",
            "  ...\n",
            "  [100.          22.73333333  46.56      ]\n",
            "  [ 90.          22.6         46.5       ]\n",
            "  [270.          22.46666667  46.59666667]]\n",
            "\n",
            " [[100.          22.86666667  45.93333333]\n",
            "  [220.          22.9         46.06      ]\n",
            "  [180.          22.93333333  46.53      ]\n",
            "  ...\n",
            "  [ 90.          22.6         46.5       ]\n",
            "  [270.          22.46666667  46.59666667]\n",
            "  [420.          22.33333333  46.99      ]]]\n",
            "shape of output data (19724, 1, 1)\n",
            "output data =>  [[[ 60.]]\n",
            "\n",
            " [[ 60.]]\n",
            "\n",
            " [[ 50.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[100.]]\n",
            "\n",
            " [[100.]]\n",
            "\n",
            " [[ 90.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_X[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CLQ4QKBratC",
        "outputId": "2cdd29fb-5999-4d09-ace6-6201bd434275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 60.        ,   6.48333333,  46.69333333],\n",
              "       [ 50.        ,   6.36666667,  46.3       ],\n",
              "       [ 50.        ,   6.25      ,  46.06666667],\n",
              "       [ 60.        ,   6.13333333,  46.33333333],\n",
              "       [ 50.        ,   6.01666667,  46.02666667],\n",
              "       [ 60.        ,   5.9       ,  45.76666667],\n",
              "       [ 60.        ,   5.91666667,  45.56      ],\n",
              "       [ 60.        ,   5.93333333,  45.5975    ],\n",
              "       [ 70.        ,   5.95      ,  46.09      ],\n",
              "       [230.        ,   5.96666667,  45.86333333]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_Y[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w6FPsA8rh0C",
        "outputId": "6c6bc9bd-2a4a-4218-f5be-98d1e4a9c42d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[50.]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets without shuffling\n",
        "input_data_train, input_data_test, y_true_train, y_true_test = train_test_split(input_data, y_true, test_size=0.2, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "GljA6a3tpjxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape X train\", input_data_train.shape)\n",
        "print(\"shape y train\", y_true_train.shape)\n",
        "print(\"shape X test\", input_data_test.shape)\n",
        "print(\"shape y test\", y_true_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtYy2gJIjuzz",
        "outputId": "51dc36b9-4499-483e-fc9f-93abcaf4755c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape X train (15778, 10, 3)\n",
            "shape y train (15778, 3, 3)\n",
            "shape X test (3945, 10, 3)\n",
            "shape y test (3945, 3, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape X train\", input_data_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl0ph27CmLmv",
        "outputId": "15396b52-a43b-4b82-d933-2902b21afe3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape X train [[ 6.6        47.59666667 63.        ]\n",
            " [ 6.48333333 46.69333333 59.16666667]\n",
            " [ 6.36666667 46.3        55.33333333]\n",
            " [ 6.25       46.06666667 51.5       ]\n",
            " [ 6.13333333 46.33333333 47.66666667]\n",
            " [ 6.01666667 46.02666667 43.83333333]\n",
            " [ 5.9        45.76666667 40.        ]\n",
            " [ 5.91666667 45.56       40.        ]\n",
            " [ 5.93333333 45.5975     40.        ]\n",
            " [ 5.95       46.09       40.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"shape y train\", y_true_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suGRW9FRmRmn",
        "outputId": "541d20a8-9716-49c5-c786-c36d3a8c2e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape y train [[ 5.96666667 45.86333333 40.        ]\n",
            " [ 5.98333333 46.39666667 40.        ]\n",
            " [ 6.         48.         40.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom loss\n",
        "def custom_loss(y_true, y_pred):\n",
        "    loss_x1 = K.mean(K.square(y_pred[:, :, 0] - y_true[:, :, 0]))\n",
        "    loss_x2 = K.mean(K.square(y_pred[:, :, 1] - y_true[:, :, 1]))\n",
        "    loss_x3 = K.mean(K.square(y_pred[:, :, 2] - y_true[:, :, 2]))\n",
        "    total_loss = loss_x1 + loss_x2 + loss_x3\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "R9qbJV5vjpE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "input_layer = Input(shape=(time_steps, X_scaled.shape[1]))\n",
        "x = LSTM(64, return_sequences=True)(input_layer)\n",
        "x = LSTM(64)(x)\n",
        "\n",
        "# Build the model with 3 units in the output layer to match the number of features\n",
        "output_layer = Dense(3)(x)\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model with the custom loss function\n",
        "model.compile(optimizer=Adam(), loss=custom_loss)\n",
        "\n",
        "# Cast y_true to float32 to match the data type of y_pred\n",
        "y_true_train = y_true_train.astype('float32')\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(input_data_train, y_true_train, epochs=3, batch_size=32)"
      ],
      "metadata": {
        "id": "Fvwswy-njrnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation of the notion**"
      ],
      "metadata": {
        "id": "BaQAwBKM3LZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {'_tempm': [72, 73, 74, 75, 76, 77, 78, 79, 80, 81],\n",
        "        'humidity': [50, 52, 55, 56, 58, 60, 62, 63, 65, 68]}\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "0I7EJmY83Ivz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 4  # Number of time steps in each sequence\n"
      ],
      "metadata": {
        "id": "wUNgeuHF3bxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequences and corresponding labels\n",
        "sequences = []\n",
        "labels = []\n",
        "for i in range(len(df) - sequence_length):\n",
        "    seq = df[i:i+sequence_length]\n",
        "    label = df[i+sequence_length][1]  # '_tempm' column index\n",
        "    sequences.append(seq)\n",
        "    labels.append(label)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "sequences = np.array(sequences)\n",
        "labels = np.array(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "kni8KhmQ3XxS",
        "outputId": "75ae37aa-e942-4831-e6ba-8a5a6cac009b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 4",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-06fd33fc2a94>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# '_tempm' column index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 4"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {'_tempm': [72, 73, 74, 75, 76, 77, 78, 79, 80, 81],\n",
        "        'humidity': [50, 52, 55, 56, 58, 60, 62, 63, 65, 68]}\n",
        "df = pd.DataFrame(data)\n",
        "sequence_length = 4  # Number of time steps in each sequence\n",
        "\n",
        "# Create sequences and corresponding labels\n",
        "sequences = []\n",
        "labels = []\n",
        "\n",
        "for i in range(len(df) - sequence_length):\n",
        "    seq = df[i:i+sequence_length]\n",
        "    label = df[i+sequence_length][1]  # 'humidity' column index\n",
        "    sequences.append(seq)\n",
        "    labels.append(label)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "sequences = np.array(sequences)\n",
        "labels = np.array(labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "7Tb5nU2k4EuC",
        "outputId": "3efcec53-bd45-46cd-89de-332cbeaead64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 4",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-dc728ebb81d5>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# 'humidity' column index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 4"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a synthetic dataset with 7 columns (you can replace this with your real dataset)\n",
        "num_rows = 100\n",
        "num_columns = 7\n",
        "synthetic_data = np.random.rand(num_rows, num_columns)\n",
        "\n",
        "# Define the sequence length\n",
        "sequence_length = 5\n",
        "\n",
        "# Create sequences and corresponding labels\n",
        "sequences = []\n",
        "labels = []\n",
        "\n",
        "for i in range(len(synthetic_data) - sequence_length):\n",
        "    # Create a sequence of length 'sequence_length' from 'synthetic_data'\n",
        "    seq = synthetic_data[i:i+sequence_length]\n",
        "\n",
        "    # Extract the label from the 7th column (assuming 0-based indexing)\n",
        "    label = synthetic_data[i+sequence_length][6]\n",
        "\n",
        "    # Append the sequence and label to their respective lists\n",
        "    sequences.append(seq)\n",
        "    labels.append(label)\n",
        "\n",
        "# Convert the lists to NumPy arrays for further processing\n",
        "sequences = np.array(sequences)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Print the generated sequences and labels for illustration\n",
        "print(\"Sequences:\")\n",
        "print(sequences)\n",
        "print(\"Labels:\")\n",
        "print(labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kXwuRwt4lJx",
        "outputId": "8c6916a8-2116-4f55-ad61-d34b88e49448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequences:\n",
            "[[[0.3959976  0.68271796 0.67950056 ... 0.9176165  0.28581863 0.55062091]\n",
            "  [0.45234044 0.77326686 0.52525395 ... 0.5200104  0.94191259 0.97990763]\n",
            "  [0.40302592 0.63691622 0.28331981 ... 0.79557395 0.58954641 0.09583094]\n",
            "  [0.04241328 0.0679106  0.76058118 ... 0.7389199  0.15433037 0.83866857]\n",
            "  [0.06395503 0.14158878 0.3940531  ... 0.46276749 0.82175677 0.38595353]]\n",
            "\n",
            " [[0.45234044 0.77326686 0.52525395 ... 0.5200104  0.94191259 0.97990763]\n",
            "  [0.40302592 0.63691622 0.28331981 ... 0.79557395 0.58954641 0.09583094]\n",
            "  [0.04241328 0.0679106  0.76058118 ... 0.7389199  0.15433037 0.83866857]\n",
            "  [0.06395503 0.14158878 0.3940531  ... 0.46276749 0.82175677 0.38595353]\n",
            "  [0.5128872  0.25797919 0.99670109 ... 0.3743473  0.78548253 0.39713662]]\n",
            "\n",
            " [[0.40302592 0.63691622 0.28331981 ... 0.79557395 0.58954641 0.09583094]\n",
            "  [0.04241328 0.0679106  0.76058118 ... 0.7389199  0.15433037 0.83866857]\n",
            "  [0.06395503 0.14158878 0.3940531  ... 0.46276749 0.82175677 0.38595353]\n",
            "  [0.5128872  0.25797919 0.99670109 ... 0.3743473  0.78548253 0.39713662]\n",
            "  [0.4673725  0.62130583 0.749979   ... 0.62934636 0.75007009 0.4853553 ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.5775342  0.63227653 0.75316749 ... 0.00583198 0.25372713 0.81725185]\n",
            "  [0.49761644 0.71935245 0.27387247 ... 0.78060923 0.17668359 0.37883125]\n",
            "  [0.39279199 0.28436879 0.74376737 ... 0.09890104 0.08662089 0.78558379]\n",
            "  [0.87716729 0.51601399 0.21422463 ... 0.90033908 0.17692738 0.88608466]\n",
            "  [0.2294973  0.7916162  0.68811443 ... 0.07494935 0.68450547 0.05974296]]\n",
            "\n",
            " [[0.49761644 0.71935245 0.27387247 ... 0.78060923 0.17668359 0.37883125]\n",
            "  [0.39279199 0.28436879 0.74376737 ... 0.09890104 0.08662089 0.78558379]\n",
            "  [0.87716729 0.51601399 0.21422463 ... 0.90033908 0.17692738 0.88608466]\n",
            "  [0.2294973  0.7916162  0.68811443 ... 0.07494935 0.68450547 0.05974296]\n",
            "  [0.92503117 0.36585197 0.14516545 ... 0.67604826 0.56371004 0.31577374]]\n",
            "\n",
            " [[0.39279199 0.28436879 0.74376737 ... 0.09890104 0.08662089 0.78558379]\n",
            "  [0.87716729 0.51601399 0.21422463 ... 0.90033908 0.17692738 0.88608466]\n",
            "  [0.2294973  0.7916162  0.68811443 ... 0.07494935 0.68450547 0.05974296]\n",
            "  [0.92503117 0.36585197 0.14516545 ... 0.67604826 0.56371004 0.31577374]\n",
            "  [0.9473435  0.33188767 0.23961547 ... 0.82880287 0.69139732 0.29771252]]]\n",
            "Labels:\n",
            "[0.39713662 0.4853553  0.62689055 0.13905203 0.38984867 0.87271816\n",
            " 0.1565604  0.2895232  0.1250519  0.8155724  0.08753937 0.71216219\n",
            " 0.57174998 0.48816758 0.63579098 0.31230332 0.10056002 0.9079316\n",
            " 0.26825039 0.12981425 0.98847945 0.62881655 0.2956891  0.2712459\n",
            " 0.91187196 0.69708099 0.5171468  0.83436279 0.72177797 0.69498863\n",
            " 0.34128433 0.23038974 0.00538591 0.26461927 0.44736485 0.640854\n",
            " 0.42860818 0.2714946  0.6411571  0.38328283 0.22446432 0.80020329\n",
            " 0.8503838  0.87922216 0.70287161 0.49099305 0.33382162 0.15580255\n",
            " 0.18620292 0.13270463 0.30896411 0.93675996 0.49514427 0.91141281\n",
            " 0.1328505  0.49151556 0.56401659 0.69688248 0.68366165 0.78603602\n",
            " 0.82493683 0.08924084 0.67644441 0.900009   0.98758001 0.19652661\n",
            " 0.69025693 0.62289234 0.75691441 0.95095883 0.78889146 0.56333636\n",
            " 0.04488292 0.94374172 0.81291666 0.64340816 0.25250796 0.26957031\n",
            " 0.41558054 0.83792971 0.94529122 0.54982764 0.44265774 0.83162218\n",
            " 0.59125073 0.01523694 0.39900825 0.81725185 0.37883125 0.78558379\n",
            " 0.88608466 0.05974296 0.31577374 0.29771252 0.96952763]\n"
          ]
        }
      ]
    }
  ]
}