{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSeFmKrdWWXl4x1T/X9IwI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cross_feature_relationships_loss(y_true, *y_preds):\n",
        "    time_steps = len(y_true)\n",
        "    num_features = len(y_preds)\n",
        "\n",
        "    loss = 0\n",
        "    for t in range(time_steps - 1):\n",
        "        for i in range(num_features):\n",
        "            for j in range(i + 1, num_features):\n",
        "                loss += (y_preds[i][t] - y_preds[j][t])**2\n",
        "                loss += (y_preds[i][t] - y_preds[j][t + 1])**2\n",
        "                loss += (y_preds[i][t + 1] - y_preds[j][t + 1])**2\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Simulated data\n",
        "y_true = np.array([15, 18, 21])\n",
        "X1 = np.array([2, 3, 4])\n",
        "X2 = np.array([5, 6, 7])\n",
        "X3 = np.array([8, 9, 10])\n",
        "\n",
        "# Predicted values\n",
        "y_pred_1 = np.array([3, 4, 5])\n",
        "y_pred_2 = np.array([6, 8, 10])\n",
        "y_pred_3 = np.array([9, 10, 11])\n",
        "\n",
        "# Calculate Cross-Feature Relationships Loss\n",
        "loss = cross_feature_relationships_loss(y_true, y_pred_1, y_pred_2, y_pred_3)\n",
        "\n",
        "print(\"Cross-Feature Relationships Loss:\", loss)\n"
      ],
      "metadata": {
        "id": "TS820prZbBcm",
        "outputId": "85e45000-9dd5-4c26-e508-41ee255a8002",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Feature Relationships Loss: 412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Simulated training data\n",
        "X_train = np.array([[2, 5, 8], [3, 6, 9], [4, 7, 10]])\n",
        "y_train = np.array([[15, 18, 21]])\n",
        "\n",
        "# Simulated test data\n",
        "X_test = np.array([[3, 6, 9]])\n",
        "\n",
        "# Define the custom loss function (Cross-Feature Relationships Loss)\n",
        "def cross_feature_relationships_loss(y_true, *y_preds):\n",
        "    time_steps = len(y_true)\n",
        "    num_features = len(y_preds)\n",
        "\n",
        "    loss = 0\n",
        "    for t in range(time_steps - 1):\n",
        "        for i in range(num_features):\n",
        "            for j in range(i + 1, num_features):\n",
        "                for k in range(t, t + 2):  # considering adjacent time steps\n",
        "                    loss += (y_preds[i][k] - y_preds[j][k])**2\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(3,)),  # Assuming 3 time steps\n",
        "    tf.keras.layers.Dense(1, activation='linear'),  # Output layer for feature 1\n",
        "    tf.keras.layers.Dense(1, activation='linear'),  # Output layer for feature 2\n",
        "])\n",
        "\n",
        "# Compile the model with the custom loss\n",
        "model.compile(optimizer='adam', loss=cross_feature_relationships_loss)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=1000, verbose=0)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the loss\n",
        "y_true = np.array([[18, 21, 24]])  # Simulated true values for illustration\n",
        "loss_value = cross_feature_relationships_loss(y_true, *y_pred).numpy()\n",
        "\n",
        "print(f\"Predicted values:\\n{y_pred}\")\n",
        "print(f\"Loss value: {loss_value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "aMFCDBpSGl2w",
        "outputId": "8f9e4328-2b15-492c-ffd4-911404c434df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7c3954ac15e1>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1958\u001b[0m             )\n\u001b[1;32m   1959\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 3\n  y sizes: 1\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Simulated training data\n",
        "X_train = np.array([[2, 5, 8], [3, 6, 9], [4, 7, 10]])\n",
        "y_train = np.array([[15, 18]])\n",
        "\n",
        "# Simulated test data\n",
        "X_test = np.array([[3, 6, 9]])\n",
        "\n",
        "# Define the custom loss function (Cross-Feature Relationships Loss)\n",
        "def cross_feature_relationships_loss(y_true, *y_preds):\n",
        "    time_steps = len(y_true)\n",
        "    num_features = len(y_preds)\n",
        "\n",
        "    loss = 0\n",
        "    for t in range(time_steps - 1):\n",
        "        for i in range(num_features):\n",
        "            for j in range(i + 1, num_features):\n",
        "                for k in range(t, t + 2):  # considering adjacent time steps\n",
        "                    loss += (y_preds[i][k] - y_preds[j][k])**2\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(3,)),  # Assuming 3 time steps\n",
        "    tf.keras.layers.Dense(1, activation='linear'),  # Output layer for feature 1\n",
        "    tf.keras.layers.Dense(1, activation='linear'),  # Output layer for feature 2\n",
        "])\n",
        "\n",
        "# Compile the model with the custom loss\n",
        "model.compile(optimizer='adam', loss=cross_feature_relationships_loss)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=1000, verbose=0)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the loss\n",
        "y_true = np.array([[18, 21]])  # Simulated true values for illustration\n",
        "loss_value = cross_feature_relationships_loss(y_true, *y_pred).numpy()\n",
        "\n",
        "print(f\"Predicted values:\\n{y_pred}\")\n",
        "print(f\"Loss value: {loss_value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "fAVka34-HCL3",
        "outputId": "4b3f261f-460d-4235-d37a-dc7dfcc0e7fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-cdead8ef6288>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1958\u001b[0m             )\n\u001b[1;32m   1959\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 3\n  y sizes: 1\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Simulated training data\n",
        "X_train = np.array([[2, 5, 8], [3, 6, 9], [4, 7, 10]])\n",
        "y_train = np.array([[15, 18]])\n",
        "\n",
        "# Simulated test data\n",
        "X_test = np.array([[3, 6, 9]])\n",
        "\n",
        "# Reshape y_train to match the model's output shape\n",
        "y_train = y_train.reshape((y_train.shape[0], y_train.shape[1], 1))\n",
        "\n",
        "# Define the custom loss function (Cross-Feature Relationships Loss)\n",
        "def cross_feature_relationships_loss(y_true, *y_preds):\n",
        "    time_steps = len(y_true[0])\n",
        "    num_features = len(y_preds)\n",
        "\n",
        "    loss = 0\n",
        "    for t in range(time_steps - 1):\n",
        "        for i in range(num_features):\n",
        "            for j in range(i + 1, num_features):\n",
        "                for k in range(t, t + 2):  # considering adjacent time steps\n",
        "                    loss += (y_preds[i][k] - y_preds[j][k])**2\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(3,)),  # Assuming 3 time steps\n",
        "    tf.keras.layers.Dense(1, activation='linear'),  # Output layer for feature 1\n",
        "    tf.keras.layers.Dense(1, activation='linear'),  # Output layer for feature 2\n",
        "])\n",
        "\n",
        "# Compile the model with the custom loss\n",
        "model.compile(optimizer='adam', loss=cross_feature_relationships_loss)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=1000, verbose=0)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the loss\n",
        "y_true = np.array([[18, 21]])  # Simulated true values for illustration\n",
        "loss_value = cross_feature_relationships_loss(y_true, *y_pred.squeeze()).numpy()\n",
        "\n",
        "print(f\"Predicted values:\\n{y_pred}\")\n",
        "print(f\"Loss value: {loss_value}\")\n"
      ],
      "metadata": {
        "id": "KE0lcyzeHOkn",
        "outputId": "24133f47-2669-4148-859c-2980321a4c2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e698897ace45>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1958\u001b[0m             )\n\u001b[1;32m   1959\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 3\n  y sizes: 1\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install skorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gRcGP0os5pb",
        "outputId": "95c29543-eb3d-443c-ee06-66db019aceb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting skorch\n",
            "  Downloading skorch-0.15.0-py3-none-any.whl (239 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/239.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/239.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.3/239.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.11.4)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from skorch) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (4.66.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (3.2.0)\n",
            "Installing collected packages: skorch\n",
            "Successfully installed skorch-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skorch import NeuralNetRegressor\n",
        "from skorch.callbacks import EpochScoring\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"train.csv\", parse_dates=[\"Date\"], index_col=[0])\n",
        "\n",
        "# Train-test split\n",
        "test_split = round(len(df) * 0.20)\n",
        "df_for_training = df[:-test_split]\n",
        "df_for_testing = df[-test_split:]\n",
        "\n",
        "# Scaling\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "df_for_training_scaled_1feature = scaler.fit_transform(df_for_training.iloc[:, :1])\n",
        "df_for_testing_scaled_1feature = scaler.transform(df_for_testing.iloc[:, :1])\n",
        "\n",
        "# Function to create input and output pairs\n",
        "def createXY(dataset, n_past):\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "    for i in range(n_past, len(dataset)):\n",
        "        dataX.append(dataset[i - n_past:i, :])\n",
        "        dataY.append(dataset[i, 0])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "# Define epochs and batch_size\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "# Create input-output pairs\n",
        "n_past = 10  # Choose an appropriate value for your sequence length\n",
        "trainX_1feature, trainY_1feature = createXY(df_for_training_scaled_1feature, n_past)\n",
        "testX_1feature, testY_1feature = createXY(df_for_testing_scaled_1feature, n_past)\n",
        "\n",
        "# Define PyTorch model\n",
        "class TimeSeriesModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=64, output_size=1):\n",
        "        super(TimeSeriesModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        output = self.fc(lstm_out[:, -1, :])  # Take the last time step's output\n",
        "        return output\n",
        "\n",
        "# Create a skorch NeuralNetRegressor\n",
        "net = NeuralNetRegressor(\n",
        "    module=TimeSeriesModel,\n",
        "    module__input_size=n_past,\n",
        "    module__output_size=1,\n",
        "    criterion=nn.MSELoss,\n",
        "    optimizer=Adam,\n",
        "    max_epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    train_split=None,  # Set to None for time series data\n",
        "    iterator_train__shuffle=False,  # Do not shuffle time series data\n",
        "    callbacks=[EpochScoring('valid_loss', use_caching=True)],  # Add a callback to track validation loss\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "net.fit(trainX_1feature.astype(np.float32), trainY_1feature.astype(np.float32))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8ikm45UtUs9",
        "outputId": "49f96796-587d-4c44-f5a3-a6f7ecefa0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.0188\u001b[0m  1.0227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      2        0.0207  0.9419\n",
            "      3        0.0205  0.6485\n",
            "      4        0.0191  0.6892\n",
            "      5        0.0233  0.8159\n",
            "      6        0.0245  0.9401\n",
            "      7        0.0265  0.9366\n",
            "      8        0.0273  0.9092\n",
            "      9        0.0299  0.6885\n",
            "     10        0.0289  0.6317\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n",
              "  module_=TimeSeriesModel(\n",
              "    (lstm): LSTM(10, 64, batch_first=True)\n",
              "    (fc): Linear(in_features=64, out_features=1, bias=True)\n",
              "  ),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skorch import NeuralNetRegressor\n",
        "from skorch.callbacks import EpochScoring\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"train.csv\", parse_dates=[\"Date\"], index_col=[0])\n",
        "\n",
        "# Train-test split\n",
        "test_split = round(len(df) * 0.20)\n",
        "df_for_training = df[:-test_split]\n",
        "df_for_testing = df[-test_split:]\n",
        "\n",
        "# Scaling\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "df_for_training_scaled_1feature = scaler.fit_transform(df_for_training.iloc[:, :1])\n",
        "df_for_testing_scaled_1feature = scaler.transform(df_for_testing.iloc[:, :1])\n",
        "\n",
        "# Function to create input and output pairs\n",
        "def createXY(dataset, n_past):\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "    for i in range(n_past, len(dataset)):\n",
        "        dataX.append(dataset[i - n_past:i, :])\n",
        "        dataY.append(dataset[i, 0])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "# Define epochs and batch_size\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "# Create input-output pairs\n",
        "n_past = 10  # Choose an appropriate value for your sequence length\n",
        "trainX_1feature, trainY_1feature = createXY(df_for_training_scaled_1feature, n_past)\n",
        "testX_1feature, testY_1feature = createXY(df_for_testing_scaled_1feature, n_past)"
      ],
      "metadata": {
        "id": "Ltoin10Rve9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom callback to track training and validation loss\n",
        "class TrainValLossCallback(Callback):\n",
        "    def on_epoch_end(self, net, **kwargs):\n",
        "        train_loss = net.history[-1, 'train_loss']\n",
        "        valid_loss_key = f'valid_{net.history.record_targets[0]}'  # Combining scorer name with \"valid_\"\n",
        "        valid_loss = net.history[-1, valid_loss_key]\n",
        "        print(f'Epoch {net.epoch} - Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}')"
      ],
      "metadata": {
        "id": "VwXhoVw6vX_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a skorch NeuralNetRegressor\n",
        "net = NeuralNetRegressor(\n",
        "    module=TimeSeriesModel,\n",
        "    module__input_size=n_past,\n",
        "    module__output_size=1,\n",
        "    criterion=nn.MSELoss,\n",
        "    optimizer=Adam,\n",
        "    max_epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    train_split=None,  # Set to None for time series data\n",
        "    iterator_train__shuffle=False,  # Do not shuffle time series data\n",
        "    callbacks=[EpochScoring('valid_loss', use_caching=True)],  # Add a callback to track validation loss\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "net.fit(trainX_1feature.astype(np.float32), trainY_1feature.astype(np.float32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rpz9iE_vkoi",
        "outputId": "c6fdd3a3-e9f5-4585-b181-14fc08295dc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  epoch    train_loss     dur\n",
            "-------  ------------  ------\n",
            "      1        \u001b[36m0.0268\u001b[0m  0.6894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      2        \u001b[36m0.0184\u001b[0m  0.6289\n",
            "      3        \u001b[36m0.0182\u001b[0m  0.7142\n",
            "      4        0.0203  0.6532\n",
            "      5        0.0197  0.6489\n",
            "      6        0.0235  0.6183\n",
            "      7        0.0233  0.6516\n",
            "      8        0.0252  0.6627\n",
            "      9        0.0289  0.5930\n",
            "     10        0.0267  0.5882\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n",
              "  module_=TimeSeriesModel(\n",
              "    (lstm): LSTM(10, 64, batch_first=True)\n",
              "    (fc): Linear(in_features=64, out_features=1, bias=True)\n",
              "  ),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "final_loss = net.score(testX_1feature.astype(np.float32), testY_1feature.astype(np.float32))\n",
        "print(\"Final Loss:\", final_loss)"
      ],
      "metadata": {
        "id": "OV8OiY4fvyKa",
        "outputId": "5d6aac85-4168-445d-8706-a63f48a3c779",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Loss: -16.5798333300636\n"
          ]
        }
      ]
    }
  ]
}